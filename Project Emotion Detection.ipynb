{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ef741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rupam\\Desktop\\IDC 302\n"
     ]
    }
   ],
   "source": [
    "cd Desktop\\\\IDC 302"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d404d3",
   "metadata": {},
   "source": [
    "Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffbbb21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rupam\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "C:\\Users\\Rupam\\AppData\\Local\\Temp/ipykernel_2668/4096154176.py:52: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 210s 467ms/step - loss: 1.8066 - accuracy: 0.2568 - val_loss: 1.7328 - val_accuracy: 0.3263\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 190s 423ms/step - loss: 1.6474 - accuracy: 0.3540 - val_loss: 1.5597 - val_accuracy: 0.4111\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 193s 431ms/step - loss: 1.5377 - accuracy: 0.4092 - val_loss: 1.4608 - val_accuracy: 0.4434\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 189s 421ms/step - loss: 1.4616 - accuracy: 0.4386 - val_loss: 1.4003 - val_accuracy: 0.4661\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 179s 400ms/step - loss: 1.4015 - accuracy: 0.4676 - val_loss: 1.3551 - val_accuracy: 0.4876\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 179s 400ms/step - loss: 1.3497 - accuracy: 0.4854 - val_loss: 1.3072 - val_accuracy: 0.5052\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 184s 411ms/step - loss: 1.3059 - accuracy: 0.5033 - val_loss: 1.2847 - val_accuracy: 0.5114\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 179s 399ms/step - loss: 1.2654 - accuracy: 0.5201 - val_loss: 1.2444 - val_accuracy: 0.5268\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 177s 395ms/step - loss: 1.2307 - accuracy: 0.5351 - val_loss: 1.2257 - val_accuracy: 0.5364\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 178s 398ms/step - loss: 1.2067 - accuracy: 0.5418 - val_loss: 1.2004 - val_accuracy: 0.5462\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 180s 402ms/step - loss: 1.1709 - accuracy: 0.5588 - val_loss: 1.1849 - val_accuracy: 0.5509\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 268s 599ms/step - loss: 1.1506 - accuracy: 0.5707 - val_loss: 1.1708 - val_accuracy: 0.5508\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 186s 415ms/step - loss: 1.1214 - accuracy: 0.5801 - val_loss: 1.1574 - val_accuracy: 0.5636\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 179s 400ms/step - loss: 1.0961 - accuracy: 0.5896 - val_loss: 1.1415 - val_accuracy: 0.5674\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 178s 398ms/step - loss: 1.0799 - accuracy: 0.5982 - val_loss: 1.1250 - val_accuracy: 0.5724\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 181s 403ms/step - loss: 1.0561 - accuracy: 0.6043 - val_loss: 1.1181 - val_accuracy: 0.5746\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 175s 390ms/step - loss: 1.0348 - accuracy: 0.6155 - val_loss: 1.1165 - val_accuracy: 0.5781\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 175s 391ms/step - loss: 1.0097 - accuracy: 0.6246 - val_loss: 1.1310 - val_accuracy: 0.5758\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 175s 390ms/step - loss: 0.9896 - accuracy: 0.6305 - val_loss: 1.0989 - val_accuracy: 0.5844\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 175s 391ms/step - loss: 0.9709 - accuracy: 0.6411 - val_loss: 1.0897 - val_accuracy: 0.5917\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 182s 407ms/step - loss: 0.9446 - accuracy: 0.6510 - val_loss: 1.0782 - val_accuracy: 0.5972\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 177s 395ms/step - loss: 0.9224 - accuracy: 0.6606 - val_loss: 1.0723 - val_accuracy: 0.6006\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 730s 2s/step - loss: 0.8964 - accuracy: 0.6688 - val_loss: 1.0655 - val_accuracy: 0.6009\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 181s 405ms/step - loss: 0.8760 - accuracy: 0.6774 - val_loss: 1.0661 - val_accuracy: 0.6041\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 178s 398ms/step - loss: 0.8516 - accuracy: 0.6853 - val_loss: 1.0700 - val_accuracy: 0.6045\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 178s 398ms/step - loss: 0.8298 - accuracy: 0.6984 - val_loss: 1.0686 - val_accuracy: 0.6087\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 173s 387ms/step - loss: 0.8063 - accuracy: 0.7067 - val_loss: 1.0718 - val_accuracy: 0.6106\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 172s 384ms/step - loss: 0.7849 - accuracy: 0.7134 - val_loss: 1.0700 - val_accuracy: 0.6106\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 174s 388ms/step - loss: 0.7647 - accuracy: 0.7236 - val_loss: 1.0596 - val_accuracy: 0.6124\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 174s 388ms/step - loss: 0.7417 - accuracy: 0.7297 - val_loss: 1.0671 - val_accuracy: 0.6151\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 176s 394ms/step - loss: 0.7207 - accuracy: 0.7364 - val_loss: 1.0592 - val_accuracy: 0.6173\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 175s 390ms/step - loss: 0.6964 - accuracy: 0.7453 - val_loss: 1.0617 - val_accuracy: 0.6214\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 183s 409ms/step - loss: 0.6857 - accuracy: 0.7500 - val_loss: 1.0626 - val_accuracy: 0.6180\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 182s 405ms/step - loss: 0.6545 - accuracy: 0.7602 - val_loss: 1.0744 - val_accuracy: 0.6247\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 179s 399ms/step - loss: 0.6409 - accuracy: 0.7644 - val_loss: 1.0842 - val_accuracy: 0.6225\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 180s 401ms/step - loss: 0.6209 - accuracy: 0.7754 - val_loss: 1.0794 - val_accuracy: 0.6158\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 180s 401ms/step - loss: 0.6039 - accuracy: 0.7788 - val_loss: 1.0874 - val_accuracy: 0.6221\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 177s 394ms/step - loss: 0.5810 - accuracy: 0.7917 - val_loss: 1.0914 - val_accuracy: 0.6300\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 179s 398ms/step - loss: 0.5569 - accuracy: 0.7965 - val_loss: 1.1004 - val_accuracy: 0.6251\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 176s 392ms/step - loss: 0.5441 - accuracy: 0.8018 - val_loss: 1.1045 - val_accuracy: 0.6237\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 174s 388ms/step - loss: 0.5142 - accuracy: 0.8138 - val_loss: 1.1303 - val_accuracy: 0.6233\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 176s 392ms/step - loss: 0.5110 - accuracy: 0.8156 - val_loss: 1.1179 - val_accuracy: 0.6302\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 176s 392ms/step - loss: 0.4919 - accuracy: 0.8224 - val_loss: 1.1169 - val_accuracy: 0.6338\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 174s 389ms/step - loss: 0.4755 - accuracy: 0.8289 - val_loss: 1.1158 - val_accuracy: 0.6304\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 591s 1s/step - loss: 0.4647 - accuracy: 0.8337 - val_loss: 1.1419 - val_accuracy: 0.6313\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 314s 701ms/step - loss: 0.4514 - accuracy: 0.8365 - val_loss: 1.1572 - val_accuracy: 0.6328\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 224s 499ms/step - loss: 0.4347 - accuracy: 0.8416 - val_loss: 1.1756 - val_accuracy: 0.6309\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 272s 608ms/step - loss: 0.4203 - accuracy: 0.8457 - val_loss: 1.1644 - val_accuracy: 0.6325\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 202s 450ms/step - loss: 0.4082 - accuracy: 0.8533 - val_loss: 1.1531 - val_accuracy: 0.6360\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 242s 540ms/step - loss: 0.3960 - accuracy: 0.8575 - val_loss: 1.1817 - val_accuracy: 0.6348\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Initialize image data generator with rescaling\n",
    "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Preprocess all test images\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        'train',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Preprocess all train images\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "        'test',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "# create model structure\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "# Train the neural network/model\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)\n",
    "\n",
    "# save model structure in jason file\n",
    "model_json = emotion_model.to_json()\n",
    "with open(\"emotion_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# save trained model weight in .h5 file\n",
    "emotion_model.save_weights('emotion_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab60d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rupam\\Desktop\\IDC 302\n"
     ]
    }
   ],
   "source": [
    "cd Desktop\\\\IDC 302"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b140cde",
   "metadata": {},
   "source": [
    "Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d511c1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3064/3833813763.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# predict the emotions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0memotion_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memotion_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcropped_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mmaxindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotion_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memotion_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmaxindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLINE_AA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1980\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1981\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1982\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1983\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1984\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('emotion_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "emotion_model.load_weights(\"emotion_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# start the webcam feed\n",
    "#cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap = cv2.VideoCapture(\"C:\\\\Users\\\\Rupam\\\\Desktop\\\\3mead.mp4\")\n",
    "\n",
    "while True:\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    if not ret:\n",
    "        break\n",
    "    face_detector = cv2.CascadeClassifier('C:\\\\Users\\\\Rupam\\\\anaconda3\\\\Lib\\\\site-packages\\\\cv2\\\\data\\\\haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces available on camera\n",
    "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # take each face available on the camera and Preprocess it\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "        # predict the emotions\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530115b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ef747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
